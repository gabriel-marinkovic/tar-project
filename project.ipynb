{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'compile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33408\\564772903.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0m_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m#torch.set_float32_matmul_precision('high')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'compile'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, DistilBertTokenizer, DistilBertModel, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, metric\n",
    "import evaluate\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "try:\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    pass\n",
    "_=torch.compile\n",
    "\n",
    "#torch.set_float32_matmul_precision('high')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceEmbedder():\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.model     = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").to(device)\n",
    "        self.model     = torch.compile(self.model, mode=\"reduce-overhead\")\n",
    "\n",
    "    # Sentence Embedding\n",
    "    # https://www.sbert.net/examples/applications/computing-embeddings/README.html                                                             \n",
    "    def mean_pooling(self, token_embeddings, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    def extract_token_indices(self, input_ids, original_text, start_idx, end_idx):\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        start_token_idx, end_token_idx = None, len(tokens)\n",
    "        cursor = 0\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
    "                continue\n",
    "\n",
    "            cursor_after_token = cursor + len(token.replace(\"##\", \"\"))\n",
    "            while cursor_after_token < len(original_text) and original_text[cursor_after_token].isspace():\n",
    "                cursor_after_token += 1\n",
    "                \n",
    "            if cursor >= start_idx and start_token_idx is None:\n",
    "                start_token_idx = i\n",
    "            if cursor_after_token >= end_idx:\n",
    "                end_token_idx = i + 1\n",
    "                break\n",
    "            \n",
    "            cursor = cursor_after_token\n",
    "\n",
    "        assert start_token_idx and end_token_idx\n",
    "        return start_token_idx, end_token_idx\n",
    "    \n",
    "    def get_sentence_embeddings(self, sentences):\n",
    "        encoded_input = self.tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self.mean_pooling(model_output[0], encoded_input['attention_mask'])\n",
    "            return sentence_embeddings\n",
    "    \n",
    "    def get_sentence_embeddings_focus_on_substring(self, sentences, indices):\n",
    "        encoded_input = self.tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "        \n",
    "        assert model_output[\"last_hidden_state\"].shape[0] == len(sentences)\n",
    "\n",
    "        final_sentence_embeddings = []\n",
    "        for i in range(len(sentences)):\n",
    "            token_idx_start, token_idx_end = self.extract_token_indices(\n",
    "                encoded_input[\"input_ids\"][i], sentences[i], indices[i][0], indices[i][1]\n",
    "            )\n",
    "            token_embeddings = model_output[0][i, token_idx_start:token_idx_end]\n",
    "            attention_mask   = encoded_input['attention_mask'][i, token_idx_start:token_idx_end]\n",
    "            sentence_embeddings = self.mean_pooling(\n",
    "                torch.unsqueeze(token_embeddings, 0), \n",
    "                torch.unsqueeze(attention_mask,   0)\n",
    "            )\n",
    "            final_sentence_embeddings.append(torch.squeeze(sentence_embeddings, 0))\n",
    "\n",
    "        return torch.stack(final_sentence_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_base_dataset():\n",
    "    dataset = load_dataset(\"humicroedit\", \"subtask-1\")\n",
    "\n",
    "    train_df = pd.DataFrame(dataset[\"train\"])\n",
    "    val_df   = pd.DataFrame(dataset[\"validation\"])\n",
    "    test_df  = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "    dfs = [train_df, test_df, val_df]\n",
    "    for df in dfs:\n",
    "        def edit_the_headline(original, edit):\n",
    "            openIdx  = original.index(\"<\")\n",
    "            closeIdx = original.index(\"/>\") + len(\"/>\")\n",
    "            return original[:openIdx] + edit + original[closeIdx:]\n",
    "        \n",
    "        df[\"original_sentence\"] = df[\"original\"].apply(lambda s: s.replace(\"<\", \"\").replace(\"/>\", \"\"))\n",
    "        df[\"edited_sentence\"]   = df.apply(lambda row: edit_the_headline(row[\"original\"], row[\"edit\"]), axis=1)\n",
    "\n",
    "        df[\"original_word_start_idx\"] = df[\"original\"].apply(lambda s: s.index(\"<\"))        \n",
    "        df[\"original_word_end_idx\"]   = df[\"original\"].apply(lambda s: s.index(\"/>\") - 1)\n",
    "\n",
    "        df[\"edited_word_start_idx\"] = df[\"original\"].apply(lambda s: s.index(\"<\"))\n",
    "        df[\"edited_word_end_idx\"]   = df.apply(lambda row: row[\"edited_word_start_idx\"] + len(row[\"edit\"]), axis=1)\n",
    "\n",
    "        df[\"all_scores\"]       = df[\"grades\"].apply(lambda s: sorted([int(c) for c in s]))\n",
    "        df[\"normalized_score\"] = df[\"meanGrade\"] / 3.0\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Tensor of shape (row_count, 2, embedding_dimensions).\n",
    "# dim=1 signifies the original and the edited sentence (0 original, 1 edited).\n",
    "def precompute_embeddings(df, embedder, compute_embeddings_batch_size=None, device=\"cpu\"):\n",
    "    original_embeddings = []\n",
    "    edited_embeddings   = []\n",
    "    \n",
    "    if compute_embeddings_batch_size is None:\n",
    "        compute_embeddings_batch_size = len(df.index)\n",
    "\n",
    "    for i in range(0, len(df.index), compute_embeddings_batch_size):\n",
    "        rows = df[i:i + compute_embeddings_batch_size]\n",
    "        original_embeddings.append(embedder.get_sentence_embeddings(rows[\"original_sentence\"].to_list()).to(device))\n",
    "        edited_embeddings  .append(embedder.get_sentence_embeddings(rows[\"edited_sentence\"]  .to_list()).to(device))\n",
    "        \n",
    "    return torch.stack((\n",
    "        torch.cat(original_embeddings), \n",
    "        torch.cat(edited_embeddings)\n",
    "    ), dim=1)\n",
    "\n",
    "def precompute_embeddings_focus_on_edit_word(df, embedder, compute_embeddings_batch_size=None, device=\"cpu\"):\n",
    "    original_embeddings = []\n",
    "    edited_embeddings   = []\n",
    "    \n",
    "    if compute_embeddings_batch_size is None:\n",
    "        compute_embeddings_batch_size = len(df.index)\n",
    "        \n",
    "    for i in range(0, len(df.index), compute_embeddings_batch_size):\n",
    "        rows = df[i:i + compute_embeddings_batch_size]\n",
    "\n",
    "        original = embedder.get_sentence_embeddings_focus_on_substring(\n",
    "            list(rows[\"original_sentence\"]),\n",
    "            list(zip(rows[\"original_word_start_idx\"], rows[\"original_word_end_idx\"]))\n",
    "        )\n",
    "        edited = embedder.get_sentence_embeddings_focus_on_substring(\n",
    "            list(rows[\"edited_sentence\"]),\n",
    "            list(zip(rows[\"edited_word_start_idx\"], rows[\"edited_word_end_idx\"]))\n",
    "        )\n",
    "\n",
    "        original_embeddings.append(original.to(device))\n",
    "        edited_embeddings  .append(edited  .to(device))\n",
    "        \n",
    "    return torch.stack((\n",
    "        torch.cat(original_embeddings), \n",
    "        torch.cat(edited_embeddings)\n",
    "    ), dim=1)\n",
    "\n",
    "class SentenceEmbeddingsDataset(Dataset):\n",
    "    def __init__(self, dataframe, precomputed_embeddings, device=\"cpu\"):\n",
    "        self.df         = dataframe\n",
    "        self.embeddings = precomputed_embeddings\n",
    "        print(f\"{self.embeddings.shape=}\")\n",
    "\n",
    "        score_counts = torch.zeros(len(self.df.index), 4)\n",
    "        for i, scores in self.df[\"all_scores\"].items():\n",
    "            for score in scores:\n",
    "                score_counts[i, score] += 1\n",
    "        self.score_counts = score_counts.to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.score_counts[idx]\n",
    "    \n",
    "class TokenizedSentencesDataset(Dataset):\n",
    "    def __init__(self, dataframe, device=\"cpu\"):\n",
    "        self.df = dataframe\n",
    "        \n",
    "        score_counts = torch.zeros(len(self.df.index), 4)\n",
    "        for i, scores in self.df[\"all_scores\"].items():\n",
    "            for score in scores:\n",
    "                score_counts[i, score] += 1\n",
    "        self.score_counts = score_counts.to(device)\n",
    "\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', use_fast=True)\n",
    "        output = tokenizer(\n",
    "            self.df[\"original_sentence\"].tolist(),\n",
    "            self.df[\"edited_sentence\"].tolist(), \n",
    "            return_tensors='pt', truncation=True, padding=True\n",
    "        ).to(device)\n",
    "        \n",
    "        self.input_ids      = output[\"input_ids\"]\n",
    "        self.attention_mask = output[\"attention_mask\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\":      self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "        }, self.score_counts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'compile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33408\\1814556325.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcached_sentence_embedder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentenceEmbedder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcached_base_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_base_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33408\\3474663173.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sentence-transformers/all-MiniLM-L6-v2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sentence-transformers/all-MiniLM-L6-v2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"reduce-overhead\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Sentence Embedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'compile'"
     ]
    }
   ],
   "source": [
    "cached_sentence_embedder = SentenceEmbedder()\n",
    "cached_base_dataset = make_base_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_dataset_naive_sentence_embeddings = [\n",
    "    SentenceEmbeddingsDataset(\n",
    "        df, \n",
    "        precompute_embeddings(df, cached_sentence_embedder, compute_embeddings_batch_size=512, device=device),\n",
    "        device=device\n",
    "    )\n",
    "    for df in cached_base_dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_dataset_edited_words_embeddings = [\n",
    "    SentenceEmbeddingsDataset(\n",
    "        df, \n",
    "        precompute_embeddings_focus_on_edit_word(df, cached_sentence_embedder, compute_embeddings_batch_size=512, device=device),\n",
    "        device=device\n",
    "    )\n",
    "    for df in cached_base_dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_dataset_tokenized_sentences = [TokenizedSentencesDataset(df, device=device) for df in cached_base_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute '_dynamo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_33408\\684184578.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[0mexperiment_predict_mean_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcached_dataset_naive_sentence_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute '_dynamo'"
     ]
    }
   ],
   "source": [
    "def experiment_predict_mean_score(train_dataset, val_dataset, test_dataset):\n",
    "    batch_size   = 2048\n",
    "    num_epochs   = 200\n",
    "    lr           = 1e-4\n",
    "    weight_decay = 0\n",
    "\n",
    "\n",
    "    score_weights = torch.arange(0, 4, dtype=torch.float, device=device).unsqueeze(1)\n",
    "    def reshape_batch(batch):\n",
    "        inputs, labels = batch\n",
    "        inputs = torch.flatten(inputs, start_dim=-2)\n",
    "        labels = labels / torch.sum(labels, dim=-1, keepdim=True)\n",
    "        actual_mean_score = torch.squeeze(labels @ score_weights) / 3\n",
    "        return inputs, actual_mean_score\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size)\n",
    "\n",
    "    one_embedding, _ = train_dataset[0]\n",
    "    input_features = torch.numel(one_embedding)\n",
    "\n",
    "    layers = []\n",
    "    #while input_features > 50:\n",
    "    #    layers.append(nn.Linear(input_features, input_features // 2))\n",
    "    #    layers.append(nn.Dropout1d(0.1))\n",
    "    #    layers.append(nn.Tanh())\n",
    "    #    input_features //= 2\n",
    "    \n",
    "    layers.append(nn.Dropout(0.3))\n",
    "    layers.append(nn.Linear(input_features, 10))\n",
    "    layers.append(nn.Tanh())\n",
    "    layers.append(nn.Linear(10, 1))\n",
    "    layers.append(nn.Sigmoid())\n",
    "\n",
    "    model = nn.Sequential(*layers)\n",
    "    model = torch.compile(model.to(device))\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader, 0):\n",
    "                inputs, labels = reshape_batch(data)\n",
    "                outputs = model(inputs).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "    print(f\"Test Loss: {test_loss/len(test_loader)}\")\n",
    "\n",
    "    # Excel output\n",
    "    predicted_scores = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            predicted_scores.extend(outputs.tolist())\n",
    "            \n",
    "    excel_df = test_dataset.df.copy()\n",
    "    excel_df[\"normalized_predicted_score\"] = predicted_scores\n",
    "    excel_df.to_excel(\"experiment_predict_mean_score.xlsx\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch._dynamo.reset()\n",
    "\n",
    "experiment_predict_mean_score(*cached_dataset_naive_sentence_embeddings)\n",
    "#experiment_predict_mean_score(*cached_dataset_edited_words_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_predict_score_distribution(train_dataset, val_dataset, test_dataset):\n",
    "    batch_size   = 2048\n",
    "    num_epochs   = 500\n",
    "    lr           = 1e-4\n",
    "    weight_decay = 0\n",
    "\n",
    "    def reshape_batch(batch):\n",
    "        inputs, labels = batch\n",
    "        inputs = torch.flatten(inputs, start_dim=-2)\n",
    "        labels = labels / torch.sum(labels, dim=-1, keepdim=True)\n",
    "        return inputs, labels\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size)\n",
    "\n",
    "    one_embedding, _ = train_dataset[0]\n",
    "    input_features = torch.numel(one_embedding)\n",
    "\n",
    "    layers = []\n",
    "    #while input_features > 50:\n",
    "    #    layers.append(nn.Linear(input_features, input_features // 2))\n",
    "    #    layers.append(nn.Dropout1d(0.1))\n",
    "    #    layers.append(nn.Tanh())\n",
    "    #    input_features //= 2\n",
    "    \n",
    "    layers.append(nn.Dropout(0.3))\n",
    "    layers.append(nn.Linear(input_features, 4))\n",
    "    #layers.append(nn.Tanh())\n",
    "    #layers.append(nn.Linear(10, 4)) # logits\n",
    "\n",
    "    model = nn.Sequential(*layers)\n",
    "    model = torch.compile(model.to(device))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_loader, 0):\n",
    "                    inputs, labels = reshape_batch(data)\n",
    "                    outputs = model(inputs).squeeze()\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "    print(f\"Test Loss: {test_loss/len(test_loader)}\")\n",
    "\n",
    "    # Excel output\n",
    "    predicted_scores = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            logits = model(inputs).squeeze()\n",
    "\n",
    "            p = torch.softmax(logits, dim=1)\n",
    "            score_weights = torch.arange(0, 4, dtype=torch.float, device=device).unsqueeze(1)\n",
    "            mean_score = torch.squeeze(p @ score_weights) / 3\n",
    "            predicted_scores.extend(mean_score.tolist())\n",
    "            \n",
    "    excel_df = test_dataset.df.copy()\n",
    "    excel_df[\"normalized_predicted_score\"] = predicted_scores\n",
    "    excel_df.to_excel(\"experiment_predict_score_distribution.xlsx\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch._dynamo.reset()\n",
    "\n",
    "#experiment_predict_score_distribution(*cached_dataset_edited_words_embeddings)\n",
    "experiment_predict_score_distribution(*cached_dataset_naive_sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_predict_score_distribution_finetune_distillbert(train_dataset, val_dataset, test_dataset):\n",
    "    batch_size   = 16 \n",
    "    num_epochs   = 1000\n",
    "    lr           = 1e-4\n",
    "    lr_bert      = 3e-5\n",
    "    weight_decay = 0\n",
    "\n",
    "    def reshape_batch(batch):\n",
    "        inputs, labels = batch\n",
    "        labels = labels / torch.sum(labels, dim=-1, keepdim=True)\n",
    "        return inputs, labels\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size)\n",
    "\n",
    "    distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "    distilbert = torch.compile(distilbert.to(device))\n",
    "\n",
    "    layers = [\n",
    "              nn.Linear(768, 768),\n",
    "              nn.Dropout1d(0.3),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(768, 768),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(768, 4)] # logits\n",
    "\n",
    "    model = nn.Sequential(*layers)\n",
    "    model = torch.compile(model.to(device))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam([\n",
    "        { \"params\": distilbert.parameters(), \"lr\": lr_bert },\n",
    "        { \"params\": model.parameters(),      \"lr\": lr, \"weight_decay\": weight_decay },\n",
    "    ])\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    def logits_from_inputs(distilbert, model, inputs):\n",
    "        outputs = distilbert(**inputs).last_hidden_state[:, 0, :]\n",
    "        logits = model(outputs).squeeze()\n",
    "        return logits\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            logits = logits_from_inputs(distilbert, model, inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_loader, 0):\n",
    "                    inputs, labels = reshape_batch(data)\n",
    "                    logits = logits_from_inputs(distilbert, model, inputs)\n",
    "                    loss = criterion(logits, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            logits = logits_from_inputs(distilbert, model, inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            test_loss += loss.item()\n",
    "    print(f\"Test Loss: {test_loss/len(test_loader)}\")\n",
    "\n",
    "    # Excel output\n",
    "    predicted_scores = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = reshape_batch(data)\n",
    "            logits = logits_from_inputs(distilbert, model, inputs)\n",
    "\n",
    "            p = torch.softmax(logits, dim=1)\n",
    "            score_weights = torch.arange(0, 4, dtype=torch.float, device=device).unsqueeze(1)\n",
    "            mean_score = torch.squeeze(p @ score_weights) / 3\n",
    "            predicted_scores.extend(mean_score.tolist())\n",
    "            \n",
    "    excel_df = test_dataset.df.copy()\n",
    "    excel_df[\"normalized_predicted_score\"] = predicted_scores\n",
    "    excel_df.to_excel(\"experiment_predict_score_distribution_finetune_distillbert.xlsx\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch._dynamo.reset()\n",
    "\n",
    "experiment_predict_score_distribution_finetune_distillbert(*cached_dataset_tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_predict_mean_score_finetune_distilbert_huggingface(train_df, val_df, test_df):\n",
    "    train_dataset = TokenizedSentencesDataset(train_df, device=\"cpu\")\n",
    "    val_dataset   = TokenizedSentencesDataset(val_df,   device=\"cpu\")\n",
    "    test_dataset  = TokenizedSentencesDataset(test_df,  device=\"cpu\")\n",
    "\n",
    "    batch_size   = 16\n",
    "    num_epochs   = 4\n",
    "    lr           = 5e-5\n",
    "    weight_decay = 0.0001\n",
    "\n",
    "    class NormalizedLabelsDataset(Dataset):\n",
    "        def __init__(self, base_dataset):\n",
    "            self.base_dataset = base_dataset\n",
    "        def __len__(self):\n",
    "            return len(self.base_dataset)\n",
    "        def __getitem__(self, idx):\n",
    "            inputs, labels = self.base_dataset[idx]\n",
    "            labels = labels / torch.sum(labels, dim=-1, keepdim=True)\n",
    "            score_weights     = torch.arange(0, 4, dtype=torch.float).unsqueeze(1)\n",
    "            actual_mean_score = torch.squeeze(labels @ score_weights) / 3\n",
    "            return { \"labels\": actual_mean_score, **inputs }\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased', use_fast=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=1)\n",
    "    #model = torch.compile(model.to(device))\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        \"distilbert-base-uncased-finetuned-subtask-1\",\n",
    "        evaluation_strategy = \"steps\",\n",
    "        eval_steps=200,\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps=200,\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_epochs,\n",
    "        weight_decay=weight_decay,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"mse\",\n",
    "    )\n",
    "\n",
    "    mae_metric = evaluate.load(\"mae\")\n",
    "    mse_metric = evaluate.load(\"mse\")\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions, labels = torch.tensor(predictions),  torch.tensor(labels)\n",
    "        predictions, labels = torch.squeeze(predictions), torch.squeeze(labels)\n",
    "        #p = torch.softmax(predictions, dim=1)\n",
    "        #score_weights = torch.arange(0, 4, dtype=torch.float).unsqueeze(1)\n",
    "        #predicted_mean_score = torch.squeeze(p      @ score_weights) / 3\n",
    "        #actual_mean_score    = torch.squeeze(labels @ score_weights) / 3\n",
    "        #return metric.compute(predictions=predicted_mean_score, references=actual_mean_score)\n",
    "        mse = mse_metric.compute(predictions=predictions, references=labels)\n",
    "        mae = mae_metric.compute(predictions=predictions, references=labels)\n",
    "        return { **mse, **mae }\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=NormalizedLabelsDataset(train_dataset),\n",
    "        eval_dataset=NormalizedLabelsDataset(val_dataset),\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "    out = trainer.predict(NormalizedLabelsDataset(test_dataset))\n",
    "    predictions = np.squeeze(out.predictions)\n",
    "            \n",
    "    excel_df = test_dataset.df.copy()\n",
    "    excel_df[\"normalized_predicted_score\"] = predictions\n",
    "    excel_df.to_excel(\"experiment_predict_mean_score_finetune_distillbert.xlsx\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch._dynamo.reset()\n",
    "\n",
    "experiment_predict_mean_score_finetune_distilbert_huggingface(*cached_base_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
